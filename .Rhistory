# no update required as it's already updated in the main function
return()
}else{ # if i is a composite max node: return max(in a) of Q[i,s,a]=V[a,s]+C[i,s,a]
actions <- .$Action.space(i)
#actions <- half.greedy.Action.space(i,s)
for(a in actions){.$EvaluateMaxNode(a, s)}
#print(c(encode(s),i,actions))
.$V[i,.$taxi$encode(s)] <- max(.$V[actions,.$taxi$encode(s)]+
.$C[[i]][.$taxi$encode(s),actions])
#print(.$V[i,.$taxi$encode(s)])
return()
}
}
#---------------------------------------------------
MAXQ0$Policy <- function(.,i,s){
# Epsilon-greedy policy
actions <- .$Action.space(i)
#actions <- half.greedy.Action.space(i,s)
if(runif(1) < .001){return(sample(actions,1))} # epsilon greedy
return(actions[nnet::which.is.max(.$V[actions,.$taxi$encode(s)]+
.$C[[i]][.$taxi$encode(s),actions])])
}
#---------------------------------------------------
MAXQ0$MAXQ0.learn <- function(.,i, s){
# if it's primitive
if(i <=6){
.$taxi$s <- s
.$taxi$a <- i
.$taxi$step()
.$gain <- .$gain + .$taxi$r
.$V[i,.$taxi$encode(s)] <- (1-alpha) * .$V[i,.$taxi$encode(s)] + alpha * .$taxi$r
return(1)
}else{
count = 0
while(!.$is.terminal(i,s) && .$gain > -.1000){
a <- .$Policy(i,s)
N <- .$MAXQ0.learn(a,s)
if(all(.$taxi$ss==c(0,0,0,0))) {return(count)}
.$EvaluateMaxNode(i,.$taxi$ss)
.$C[[i]][.$taxi$encode(s),a] <- (1-alpha) * .$C[[i]][.$taxi$encode(s),a] +
alpha * (DF^N) * .$V[i,.$taxi$encode(.$taxi$ss)]
count <- count + N
s <- .$taxi$ss
.$taxi$s <- .$taxi$ss
}
#if(.$gain<=-1000){.$gain=-}
#.$taxi$render()
return(count)
}
}
# #---------------------------------------------
MAXQ0$Estimate.V.C <- function(., n=100){
s0s <- list()
set.seed(12)
rewards <- c()
for(i in 1:n){
# for each run generate a random state.
s0 <- c(sample(1:5,1),sample(1:5,1),sample(1:4,1),sample(1:4,1))
#s0 <- c(3,2,4,1)
# print(s0)
.$MAXQ0.learn(7, s0)
rewards <- c(rewards, .$gain)
}
return(rewards)
}
#
#----------------------------------------------------------
learn <- as.proto(MAXQ0)
#learn <- MAXQ0$proto()
s0 <- c(3,2,1,4)
learn$MAXQ0.learn(7,s0)
learn$Policy(7,s0)
learn$gain
rewards <- learn$Estimate.V.C(50)
learn$Action.space(7)
MAXQ0$Action.space(7)
taxi.env$decode(13)
#
# o <- Estimate.V.C(500, Vestim, Cestim)
# Vestim <- o[[1]]
# Cestim <- o[[2]]
# rewards <- o[[3]]
#
# #debug(MAXQ0.learn)
#
# plot(1:length(rewards),rewards)
# max(rewards)
# min(rewards)
# #s0 = c(4,1,5,3)
# o <-MAXQ0.learn(7,s0,Vestim,Cestim,0)
# o[[5]]
# # half.greedy.Action.space <- function(i,s){
#   # This version is stricter where the action space depends on the state as well as the
#   # subtask.
#   # expects only i from 7 to 13. ie, composite actions [subtasks]
#   if(i==7){
#     if(s[3]!=5){return(c(8))
#     }else {return(c(9))}
#
#   }else if(i==8){
#     if(all(loc.indx(s[3])==s[1:2])){return(c(5))
#     }else if(s[[3]]==1){return(c(10))
#     }else if(s[[3]]==2){return(c(11))
#     }else if(s[[3]]==3){return(c(12))
#     }else if(s[[3]]==4){return(c(13))}
#
#   }else if(i==9){
#     if(all(loc.indx(s[4])==s[1:2])){return(c(6))
#     }else if(s[[4]]==1){return(c(10))
#     }else if(s[[4]]==2){return(c(11))
#     }else if(s[[4]]==3){return(c(12))
#     }else if(s[[4]]==4){return(c(13))}
#
#   }else {return(1:4)} # == else if(i is in 10:13)
# }
#
#
# o <- Estimate.V.C(500, Vestim, Cestim)
# Vestim <- o[[1]]
# Cestim <- o[[2]]
# rewards <- o[[3]]
#
# #debug(MAXQ0.learn)
#
plot(1:length(rewards),rewards)
MAXQ0$Estimate.V.C <- function(., n=100){
s0s <- list()
set.seed(12)
rewards <- c()
for(i in 1:n){
# for each run generate a random state.
s0 <- c(sample(1:5,1),sample(1:5,1),sample(1:4,1),sample(1:4,1))
#s0 <- c(3,2,4,1)
# print(s0)
.$MAXQ0.learn(7, s0)
rewards <- c(rewards, .$gain)
.$gain <- 0
}
return(rewards)
}
rewards <- learn$Estimate.V.C(50)
#
# o <- Estimate.V.C(500, Vestim, Cestim)
# Vestim <- o[[1]]
# Cestim <- o[[2]]
# rewards <- o[[3]]
#
# #debug(MAXQ0.learn)
#
plot(1:length(rewards),rewards)
min(rewards)
max(rewards)
MAXQ0$MAXQ0.learn <- function(.,i, s){
# if it's primitive
if(i <=6){
.$taxi$s <- s
.$taxi$a <- i
.$taxi$step()
.$gain <- .$gain + .$taxi$r
print(.$gain)
.$V[i,.$taxi$encode(s)] <- (1-alpha) * .$V[i,.$taxi$encode(s)] + alpha * .$taxi$r
return(1)
}else{
count = 0
while(!.$is.terminal(i,s) && .$gain > -.1000){
a <- .$Policy(i,s)
N <- .$MAXQ0.learn(a,s)
if(all(.$taxi$ss==c(0,0,0,0))) {return(count)}
.$EvaluateMaxNode(i,.$taxi$ss)
.$C[[i]][.$taxi$encode(s),a] <- (1-alpha) * .$C[[i]][.$taxi$encode(s),a] +
alpha * (DF^N) * .$V[i,.$taxi$encode(.$taxi$ss)]
count <- count + N
s <- .$taxi$ss
.$taxi$s <- .$taxi$ss
}
#if(.$gain<=-1000){.$gain=-}
#.$taxi$render()
return(count)
}
}
learn <- as.proto(MAXQ0)
learn$MAXQ0.learn(7,s0)
learn$MAXQ0.learn(7,s0)
learn$MAXQ0.learn(7,s0)
learn$MAXQ0.learn(7,s0)
learn$MAXQ0.learn(7,s0)
learn$MAXQ0.learn(7,s0)
MAXQ0$MAXQ0.learn <- function(.,i, s){
# if it's primitive
if(i <=6){
.$taxi$s <- s
.$taxi$a <- i
.$taxi$step()
.$gain <- .$gain + .$taxi$r
.$taxi$render()
.$V[i,.$taxi$encode(s)] <- (1-alpha) * .$V[i,.$taxi$encode(s)] + alpha * .$taxi$r
return(1)
}else{
count = 0
while(!.$is.terminal(i,s) && .$gain > -.1000){
a <- .$Policy(i,s)
N <- .$MAXQ0.learn(a,s)
if(all(.$taxi$ss==c(0,0,0,0))) {return(count)}
.$EvaluateMaxNode(i,.$taxi$ss)
.$C[[i]][.$taxi$encode(s),a] <- (1-alpha) * .$C[[i]][.$taxi$encode(s),a] +
alpha * (DF^N) * .$V[i,.$taxi$encode(.$taxi$ss)]
count <- count + N
s <- .$taxi$ss
.$taxi$s <- .$taxi$ss
}
#if(.$gain<=-1000){.$gain=-}
#.$taxi$render()
return(count)
}
}
learn <- as.proto(MAXQ0)
learn$MAXQ0.learn(7,s0)
learn$MAXQ0.learn(7,s0)
learn$MAXQ0.learn(7,s0)
learn$MAXQ0.learn(7,s0)
learn$MAXQ0.learn(7,s0)
learn$MAXQ0.learn(7,s0)
learn$MAXQ0.learn(7,s0)
learn$MAXQ0.learn(7,s0)
learn$MAXQ0.learn(7,s0)
learn$MAXQ0.learn(7,s0)
learn$MAXQ0.learn(7,s0)
learn$MAXQ0.learn(7,s0)
learn$MAXQ0.learn(7,s0)
learn$MAXQ0.learn(7,s0)
learn$MAXQ0.learn(7,s0)
learn$MAXQ0.learn(7,s0)
learn$MAXQ0.learn(7,s0)
learn$MAXQ0.learn(7,s0)
learn$MAXQ0.learn(7,s0)
MAXQ0$MAXQ0.learn <- function(.,i, s){
# if it's primitive
if(i <=6){
.$taxi$s <- s
.$taxi$a <- i
.$taxi$step()
.$gain <- .$gain + .$taxi$r
.$taxi$render()
.$V[i,.$taxi$encode(s)] <- (1-alpha) * .$V[i,.$taxi$encode(s)] + alpha * .$taxi$r
return(1)
}else{
count = 0
while(!.$is.terminal(i,s) && .$gain > -.1000){
print(i)
a <- .$Policy(i,s)
N <- .$MAXQ0.learn(a,s)
if(all(.$taxi$ss==c(0,0,0,0))) {return(count)}
.$EvaluateMaxNode(i,.$taxi$ss)
.$C[[i]][.$taxi$encode(s),a] <- (1-alpha) * .$C[[i]][.$taxi$encode(s),a] +
alpha * (DF^N) * .$V[i,.$taxi$encode(.$taxi$ss)]
count <- count + N
s <- .$taxi$ss
.$taxi$s <- .$taxi$ss
}
#if(.$gain<=-1000){.$gain=-}
#.$taxi$render()
return(count)
}
}
learn <- as.proto(MAXQ0)
learn$MAXQ0.learn(7,s0)
learn$MAXQ0.learn(7,s0)
learn$MAXQ0.learn(7,s0)
learn$gain
learn$is.terminal(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
learn <- as.proto(MAXQ0)
rrr <- learn$MAXQ0.learn(7,s0)
MAXQ0$MAXQ0.learn <- function(.,i, s){
# if it's primitive
if(i <=6){
.$taxi$s <- s
.$taxi$a <- i
.$taxi$step()
.$gain <- .$gain + .$taxi$r
.$taxi$render()
.$V[i,.$taxi$encode(s)] <- (1-alpha) * .$V[i,.$taxi$encode(s)] + alpha * .$taxi$r
return(1)
}else{
count = 0
while(!.$is.terminal(i,s) && .$gain > -.1000){
print(i)
a <- .$Policy(i,s)
N <- .$MAXQ0.learn(a,s)
if(all(.$taxi$ss==c(0,0,0,0))) {return(count)}
.$EvaluateMaxNode(i,.$taxi$ss)
.$C[[i]][.$taxi$encode(s),a] <- (1-alpha) * .$C[[i]][.$taxi$encode(s),a] +
alpha * (DF^N) * .$V[i,.$taxi$encode(.$taxi$ss)]
count <- count + N
s <- .$taxi$ss
.$taxi$s <- .$taxi$ss
}
#if(.$gain<=-1000){.$gain=-}
#.$taxi$render()
return(count)
}
}
learn <- as.proto(MAXQ0)
rrr <- learn$MAXQ0.learn(7,s0)
MAXQ0$MAXQ0.learn <- function(.,i, s){
# if it's primitive
print(i)
if(i <=6){
.$taxi$s <- s
.$taxi$a <- i
.$taxi$step()
.$gain <- .$gain + .$taxi$r
.$taxi$render()
.$V[i,.$taxi$encode(s)] <- (1-alpha) * .$V[i,.$taxi$encode(s)] + alpha * .$taxi$r
return(1)
}else{
count = 0
while(!.$is.terminal(i,s) && .$gain > -.1000){
print(i)
a <- .$Policy(i,s)
N <- .$MAXQ0.learn(a,s)
if(all(.$taxi$ss==c(0,0,0,0))) {return(count)}
.$EvaluateMaxNode(i,.$taxi$ss)
.$C[[i]][.$taxi$encode(s),a] <- (1-alpha) * .$C[[i]][.$taxi$encode(s),a] +
alpha * (DF^N) * .$V[i,.$taxi$encode(.$taxi$ss)]
count <- count + N
s <- .$taxi$ss
.$taxi$s <- .$taxi$ss
}
#if(.$gain<=-1000){.$gain=-}
#.$taxi$render()
return(count)
}
}
learn <- as.proto(MAXQ0)
rrr <- learn$MAXQ0.learn(7,s0)
MAXQ0$MAXQ0.learn <- function(.,i, s){
# if it's primitive
if(i <=6){
.$taxi$s <- s
.$taxi$a <- i
.$taxi$step()
.$gain <- .$gain + .$taxi$r
.$taxi$render()
.$V[i,.$taxi$encode(s)] <- (1-alpha) * .$V[i,.$taxi$encode(s)] + alpha * .$taxi$r
return(1)
}else{print(i)
count = 0
while(!.$is.terminal(i,s) && .$gain > -.1000){
print(i)
a <- .$Policy(i,s)
N <- .$MAXQ0.learn(a,s)
if(all(.$taxi$ss==c(0,0,0,0))) {return(count)}
.$EvaluateMaxNode(i,.$taxi$ss)
.$C[[i]][.$taxi$encode(s),a] <- (1-alpha) * .$C[[i]][.$taxi$encode(s),a] +
alpha * (DF^N) * .$V[i,.$taxi$encode(.$taxi$ss)]
count <- count + N
s <- .$taxi$ss
.$taxi$s <- .$taxi$ss
}
#if(.$gain<=-1000){.$gain=-}
#.$taxi$render()
return(count)
}
}
learn <- as.proto(MAXQ0)
rrr <- learn$MAXQ0.learn(7,s0)
learn$is.terminal(i,s)
learn$is.terminal(i,s0)
!learn$is.terminal(i,s0)
MAXQ0$MAXQ0.learn <- function(.,i, s){
# if it's primitive
if(i <=6){
.$taxi$s <- s
.$taxi$a <- i
.$taxi$step()
.$gain <- .$gain + .$taxi$r
.$taxi$render()
.$V[i,.$taxi$encode(s)] <- (1-alpha) * .$V[i,.$taxi$encode(s)] + alpha * .$taxi$r
return(1)
}else{
count = 0
while(!.$is.terminal(i,s) ){
print(i)
a <- .$Policy(i,s)
N <- .$MAXQ0.learn(a,s)
if(all(.$taxi$ss==c(0,0,0,0))) {return(count)}
.$EvaluateMaxNode(i,.$taxi$ss)
.$C[[i]][.$taxi$encode(s),a] <- (1-alpha) * .$C[[i]][.$taxi$encode(s),a] +
alpha * (DF^N) * .$V[i,.$taxi$encode(.$taxi$ss)]
count <- count + N
s <- .$taxi$ss
.$taxi$s <- .$taxi$ss
}
#if(.$gain<=-1000){.$gain=-}
#.$taxi$render()
return(count)
}
}
learn <- as.proto(MAXQ0)
#learn <- MAXQ0$proto()
s0 <- c(3,2,1,4)
rrr <- learn$MAXQ0.learn(7,s0)
learn$gain
table(learn$V)
MAXQ0$MAXQ0.learn <- function(.,i, s){
# if it's primitive
if(i <=6){
.$taxi$s <- s
.$taxi$a <- i
.$taxi$step()
.$gain <- .$gain + .$taxi$r
.$V[i,.$taxi$encode(s)] <- (1-alpha) * .$V[i,.$taxi$encode(s)] + alpha * .$taxi$r
return(1)
}else{
count = 0
while(!.$is.terminal(i,s) && .$gain > -1000){
a <- .$Policy(i,s)
N <- .$MAXQ0.learn(a,s)
if(all(.$taxi$ss==c(0,0,0,0))) {return(count)}
.$EvaluateMaxNode(i,.$taxi$ss)
.$C[[i]][.$taxi$encode(s),a] <- (1-alpha) * .$C[[i]][.$taxi$encode(s),a] +
alpha * (DF^N) * .$V[i,.$taxi$encode(.$taxi$ss)]
count <- count + N
s <- .$taxi$ss
.$taxi$s <- .$taxi$ss
}
#if(.$gain<=-1000){.$gain=-}
#.$taxi$render()
return(count)
}
}
rrr <- learn$MAXQ0.learn(7,s0)
rewards <- learn$Estimate.V.C(50)
#
# o <- Estimate.V.C(500, Vestim, Cestim)
# Vestim <- o[[1]]
# Cestim <- o[[2]]
# rewards <- o[[3]]
#
# #debug(MAXQ0.learn)
#
plot(1:length(rewards),rewards)
max(rewards)
min(rewards)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
rrr <- learn$MAXQ0.learn(7,s0)
learn$gain
table(learn$V)
