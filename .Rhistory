.$taxi$update()
}
return(count)
}
# #---------------------------------------------
MAXQ0$Estimate.V.C <- function(., n=100){
# runs n episodes of MAXQ0() using random initial state at each episode
# and updating the estimates for value functions and keeping track of the rewards
rewards <- c()
for(i in 1:n){
.$init(FALSE,alpha=.2, DF=1, epsilon=1/i)
# for each run generate a random state.
s0 <- c(sample(1:5,1),sample(1:5,1),sample(1:4,1),sample(1:4,1))
.$MAXQ0(7, s0) # always start from the root subtask
rewards <- c(rewards, .$gain)
if(i%%500==0){print(paste0(i," episodes are simulated"))}
}
return(rewards)
}
MAXQ0$get.rands <- function(.){ # returns a random state
return(c(sample(1:5,1),sample(1:5,1),sample(1:4,1),sample(1:4,1)))
}
#
#
#----------------------------------------------------------
# example of running one episode
e <- as.proto(MAXQ0)
e$init() # needs to be always called after the creation of the object
learn$MAXQ0(7,e$get.rands())
e$MAXQ0(7,e$get.rands())
e$get.rands()
e$taxi$ss
library(proto)
library(crayon)
taxi.env <- proto(expr={
# avaiable variables
s = NA
ss = NA
r = NA
# available functions (defined later below)
render = NA
step = NA
hitting.wallQ = NA
encode = NA
decode = NA
loc.indx = NA
set.random.state = NA
set.seed = NA
})
taxi.env$render <- function(.){
out <- c("|R: | : :G|",
"| : | : : |",
"| : : : : |",
"| | : | : |",
"|Y| : |B: |")
substr(out[.$s[1]],2*.$s[2],2*.$s[2])='#'
if(.$s[3]!=5){
ploc <- .$loc.indx(.$s[3])
dloc <- .$loc.indx(.$s[4])
if(ploc[1]!= dloc[1]){
out[ploc[1]] = paste0(substr(out[ploc[1]],1,2*ploc[2]-1),
green(substr(out[ploc[1]],2*ploc[2],2*ploc[2])),
substr(out[ploc[1]],2*ploc[2]+1,11))
out[dloc[1]] = paste0(substr(out[dloc[1]],1,2*dloc[2]-1),
red(substr(out[dloc[1]],2*dloc[2],2*dloc[2])),
substr(out[dloc[1]],2*dloc[2]+1,11))
}else if(ploc[2]<dloc[2]){
out[ploc[1]] = paste0(substr(out[ploc[1]],1 ,2*ploc[2]-1),
green(substr(out[ploc[1]],2*ploc[2],2*ploc[2])),
substr(out[ploc[1]], 2*ploc[2]+1, 2*dloc[2]-1),
red(substr(out[dloc[1]],2*dloc[2],2*dloc[2])),
substr(out[dloc[1]],2*dloc[2]+1,11))
}else if(ploc[2]>dloc[2]){
out[ploc[1]] = paste0(substr(out[dloc[1]],1 ,2*dloc[2]-1),
red(substr(out[dloc[1]],2*dloc[2],2*dloc[2])),
substr(out[dloc[1]], 2*dloc[2]+1, 2*ploc[2]-1),
green(substr(out[ploc[1]],2*ploc[2],2*ploc[2])),
substr(out[ploc[1]],2*ploc[2]+1,11))
}
}else{
loc <- .$loc.indx(.$s[4])
out[loc[1]] = paste0(substr(out[loc[1]],1,2*loc[2]-1),
red(substr(out[loc[1]],2*loc[2],2*loc[2])),
substr(out[loc[1]],2*loc[2]+1,11))
}
write("+---------+",stdout())
write(out,stdout())
write("+---------+",stdout())
}
taxi.env$step <- function(.,a){
.$ss <- .$s
.$r <- -1
# .$s is a vector of c(taxi_row, taxi_col, pass_loc, dest_loc)
# a is an integer from 1 to 10
# return a list of the reward and next .$s
# a next .$s of c(0,0,0,0) means that he successfully dropped off the passenger
# and the episode is over.
if(a==6){
# if successfully dropping-off the passenger
if(.$s[3]==5 && all(.$loc.indx(.$s[4])==.$s[1:2])){
.$ss <- c(0,0,0,0)
.$r <- 20
}else {.$r <- -10}
#..........
}else if(a==5){
# if successfully picking up the passenger
if(.$s[3]!=5 && all(.$loc.indx(.$s[3])==.$s[1:2])){
.$ss[3]=5
.$r = 0
}else {.$r <- -10}
#..........#North
}else if(a==1 && .$hitting.wallQ(a)==FALSE){.$ss[1]=.$ss[1]-1
#..........#South
}else if(a==2 && .$hitting.wallQ(a)==FALSE){.$ss[1]=.$ss[1]+1
#................#East
}else if(a==3 && .$hitting.wallQ(a)==FALSE){.$ss[2]=.$ss[2]+1
#...............#west
}else if(a==4 && .$hitting.wallQ(a)==FALSE){.$ss[2]=.$ss[2]-1}
return(.$r)
}
taxi.env$hitting.wallQ <- function(.,a){
# true if the action would result in hitting a wall
if( (a==1 && .$s[1]==1) || (a==2 && .$s[1]==5)){return(TRUE)
}else if (a==3){
if(.$s[2]==5){return(TRUE)}
if(((.$s[2]==1||.$s[2]==3)&&(.$s[1]==5||.$s[1]==4))||(.$s[2]==2&&(.$s[1]==1||.$s[1]==2))){return(TRUE)}
}else if(a==4){
if(.$s[2]==1){return(TRUE)}
if(((.$s[2]==2||.$s[2]==4)&&(.$s[1]==5||.$s[1]==4))||(.$s[2]==3&&(.$s[1]==2||.$s[1]==1))){return(TRUE)}
}
return(FALSE)
}
taxi.env$encode <- function(.,s){
# encode the state(a,b,c,d) where a,b,c can have values from 1 to 5 and d can have values
# from 1 to 4.
# then the encoded variable can have values between 1 and 500
s = s-1
return(4*(5*(5*s[1]+s[2])+s[3])+s[4]+1)
}
taxi.env$decode <- function(.,i){
# the inverse of encode()
i <- i-1
d <- i%%4
i <- i %/% 4
c <- i%%5
i <- i%/%5
b <- i%%5
i <- i%/%5
return(c(i+1,b+1,c+1,d+1))
}
taxi.env$loc.indx <- function(.,i){
# takes a number between 1 and 4 representing (R,G,Y,B) and returns
# the equivalent (row,col) location
if(i==1){return(c(1,1))
}else if(i==2){return(c(1,5))
}else if(i==3){return(c(5,1))
}else if(i==4){return(c(5,4))}
return(c(0,0)) # index other than 1 to 4
}
taxi.env$set.random.state <- function(.){
.$s <- c(sample(1:5,1),sample(1:5,1),sample(1:4,1),sample(1:4,1))
}
taxi.env$set.seed <- function(.,seed){set.seed(seed)}
taxi.env$update <- function(.){.$s <- .$ss}
#------------------------------------------------
# Usage :
t <- as.proto(taxi.env)
t$set.random.state(); t$render()
t$step(a=1); t$update(); t$render()
t$s <- t$decode(150); t$render()
library(nnet)
library(proto)
MAXQ0 <- proto(expr = {
# Matrices (all are defined later below)
V <- NA # holds the estimates for the value functions
C <- NA
# Variables
epsilon <- NA # for the epsilon-greedy policy
DF <- NA # the discounted factor
alpha <- NA # learning rate
gain <- 0 # sum of rewards for a single episode
taxi <- as.proto(taxi.env)
# available methods
set.seed <- NA
init <- NA
is.terminal <- NA
Action.space <- NA
EvaluateMaxNode <- NA
Policy <- NA
MAXQ0 <- NA
Estimate.V.C <- NA
get.rands <- NA
})
#----------------------------------
MAXQ0$set.seed <- function(.,seed){set.seed(seed)}
MAXQ0$init <- function(.,reset.estimates=TRUE, alpha=.2,DF=1,epsilon=.01){
if(reset.estimates == TRUE){
.$V <- matrix(0,11,500)
.$C <-list()
for(i in 1:11){
.$C[[i]] <- matrix(0,500,11)
}
}
.$alpha <- alpha
.$DF <- DF
.$gain <- 0
.$epsilon <- epsilon
.$taxi <- as.proto(taxi.env)
}
#--------------------------------------
MAXQ0$is.terminal <- function(., i, s){
# returns TRUE if s is an end state for the subtask i.
# whenver in task 7(root), 9(dropoff), or 10(navig with a parent 9), terminate
# only of the episode is done
if(all(s==c(0,0,0,0))){return(TRUE)
}else if(any(i==c(8,10)) && s[3]==5){return(TRUE) # successfully picked up the passenger
}else if(any(i==c(9,11)) && s[3]!=5){return(TRUE) # attempting to drop off when passenger is not on board
}else if(any(i==1:6)){return(TRUE) # any primitive action is excuted once
}else if(i==10 && all(s[1:2]==.$taxi$loc.indx(s[3]))){return(TRUE)#taxi ready to pick up
}else if(i==11 && all(s[1:2]==.$taxi$loc.indx(s[4]))){return(TRUE)
}else {return(FALSE)}
}
#-------------------------------------------------------------------
MAXQ0$Action.space <- function(.,i){
# expects only i from 7 to 13. ie, composite actions [subtasks]
if(i==7){return(c(8,9))
}else if(i==8){return(c(5,10))
}else if(i==9){return(c(6,11))
}else if(i==10 || i==11){return(1:4)}
}
#---------------------------------------------------
MAXQ0$EvaluateMaxNode <- function(., i, s){
# if i is primitive then no update is required then it's updated in MAXQ0()
if(any(i==1:6)){return()}
# if i is a composite max node: return max(in a) of Q[i,s,a]=V[a,s]+C[i,s,a]
actions <- .$Action.space(i)
for(a in actions){.$EvaluateMaxNode(a, s)}
.$V[i,.$taxi$encode(s)] <- max(.$V[actions,.$taxi$encode(s)]+.$C[[i]][.$taxi$encode(s),actions])
}
#---------------------------------------------------
MAXQ0$Policy <- function(.,i,s){
# Epsilon-greedy policy
actions <- .$Action.space(i)
if(runif(1) < .$epsilon){return(sample(actions,1))} # epsilon greedy
return(actions[which.max(.$V[actions,.$taxi$encode(s)]+
.$C[[i]][.$taxi$encode(s),actions])])
}
#---------------------------------------------------
MAXQ0$MAXQ0 <- function(.,i, s){
# if it's primitive
.$taxi$s <- s
.$taxi$a <- i
if(i <=6){
r <- .$taxi$step(i)
.$gain <- .$gain + r
.$V[i,.$taxi$encode(s)] <- (1-.$alpha) * .$V[i,.$taxi$encode(s)] + .$alpha * r
return(1)
}
# else if i is a composite action (subtask)
count = 0
while(!.$is.terminal(i,s)){
a <- .$Policy(i,s)
N <- .$MAXQ0(a,s)
if(all(.$taxi$ss==c(0,0,0,0))) {return(count)} # goal is achieved.
.$EvaluateMaxNode(i,.$taxi$ss)
.$C[[i]][.$taxi$encode(s),a] <- (1-.$alpha) * .$C[[i]][.$taxi$encode(s),a] +
.$alpha * (.$DF^N) * .$V[i,.$taxi$encode(.$taxi$ss)]
count <- count + N
s <- .$taxi$ss
.$taxi$update()
}
return(count)
}
# #---------------------------------------------
MAXQ0$Estimate.V.C <- function(., n=100){
# runs n episodes of MAXQ0() using random initial state at each episode
# and updating the estimates for value functions and keeping track of the rewards
rewards <- c()
for(i in 1:n){
.$init(FALSE,alpha=.2, DF=1, epsilon=1/i)
# for each run generate a random state.
s0 <- c(sample(1:5,1),sample(1:5,1),sample(1:4,1),sample(1:4,1))
.$MAXQ0(7, s0) # always start from the root subtask
rewards <- c(rewards, .$gain)
if(i%%500==0){print(paste0(i," episodes are simulated"))}
}
return(rewards)
}
MAXQ0$get.rands <- function(.){ # returns a random state
return(c(sample(1:5,1),sample(1:5,1),sample(1:4,1),sample(1:4,1)))
}
#
#
#----------------------------------------------------------
# example of running one episode
e <- as.proto(MAXQ0)
e$init() # needs to be always called after the creation of the object
e$MAXQ0(7,e$get.rands())
e$get.rands()
#-----------------------------------------------------
# running n episodes and observing the resulting rewards and value estimates.
e$set.seed(12)
e$init()
library(nnet)
library(proto)
MAXQ0 <- proto(expr = {
# Matrices (all are defined later below)
V <- NA # holds the estimates for the value functions
C <- NA
# Variables
epsilon <- NA # for the epsilon-greedy policy
DF <- NA # the discounted factor
alpha <- NA # learning rate
gain <- 0 # sum of rewards for a single episode
taxi <- as.proto(taxi.env)
# available methods
add.seed <- NA
init <- NA
is.terminal <- NA
Action.space <- NA
EvaluateMaxNode <- NA
Policy <- NA
MAXQ0 <- NA
Estimate.V.C <- NA
get.rands <- NA
})
#----------------------------------
MAXQ0$add.seed <- function(.,seed){set.seed(seed)}
MAXQ0$init <- function(.,reset.estimates=TRUE, alpha=.2,DF=1,epsilon=.01){
if(reset.estimates == TRUE){
.$V <- matrix(0,11,500)
.$C <-list()
for(i in 1:11){
.$C[[i]] <- matrix(0,500,11)
}
}
.$alpha <- alpha
.$DF <- DF
.$gain <- 0
.$epsilon <- epsilon
.$taxi <- as.proto(taxi.env)
}
#--------------------------------------
MAXQ0$is.terminal <- function(., i, s){
# returns TRUE if s is an end state for the subtask i.
# whenver in task 7(root), 9(dropoff), or 10(navig with a parent 9), terminate
# only of the episode is done
if(all(s==c(0,0,0,0))){return(TRUE)
}else if(any(i==c(8,10)) && s[3]==5){return(TRUE) # successfully picked up the passenger
}else if(any(i==c(9,11)) && s[3]!=5){return(TRUE) # attempting to drop off when passenger is not on board
}else if(any(i==1:6)){return(TRUE) # any primitive action is excuted once
}else if(i==10 && all(s[1:2]==.$taxi$loc.indx(s[3]))){return(TRUE)#taxi ready to pick up
}else if(i==11 && all(s[1:2]==.$taxi$loc.indx(s[4]))){return(TRUE)
}else {return(FALSE)}
}
#-------------------------------------------------------------------
MAXQ0$Action.space <- function(.,i){
# expects only i from 7 to 13. ie, composite actions [subtasks]
if(i==7){return(c(8,9))
}else if(i==8){return(c(5,10))
}else if(i==9){return(c(6,11))
}else if(i==10 || i==11){return(1:4)}
}
#---------------------------------------------------
MAXQ0$EvaluateMaxNode <- function(., i, s){
# if i is primitive then no update is required then it's updated in MAXQ0()
if(any(i==1:6)){return()}
# if i is a composite max node: return max(in a) of Q[i,s,a]=V[a,s]+C[i,s,a]
actions <- .$Action.space(i)
for(a in actions){.$EvaluateMaxNode(a, s)}
.$V[i,.$taxi$encode(s)] <- max(.$V[actions,.$taxi$encode(s)]+.$C[[i]][.$taxi$encode(s),actions])
}
#---------------------------------------------------
MAXQ0$Policy <- function(.,i,s){
# Epsilon-greedy policy
actions <- .$Action.space(i)
if(runif(1) < .$epsilon){return(sample(actions,1))} # epsilon greedy
return(actions[which.max(.$V[actions,.$taxi$encode(s)]+
.$C[[i]][.$taxi$encode(s),actions])])
}
#---------------------------------------------------
MAXQ0$MAXQ0 <- function(.,i, s){
# if it's primitive
.$taxi$s <- s
.$taxi$a <- i
if(i <=6){
r <- .$taxi$step(i)
.$gain <- .$gain + r
.$V[i,.$taxi$encode(s)] <- (1-.$alpha) * .$V[i,.$taxi$encode(s)] + .$alpha * r
return(1)
}
# else if i is a composite action (subtask)
count = 0
while(!.$is.terminal(i,s)){
a <- .$Policy(i,s)
N <- .$MAXQ0(a,s)
if(all(.$taxi$ss==c(0,0,0,0))) {return(count)} # goal is achieved.
.$EvaluateMaxNode(i,.$taxi$ss)
.$C[[i]][.$taxi$encode(s),a] <- (1-.$alpha) * .$C[[i]][.$taxi$encode(s),a] +
.$alpha * (.$DF^N) * .$V[i,.$taxi$encode(.$taxi$ss)]
count <- count + N
s <- .$taxi$ss
.$taxi$update()
}
return(count)
}
# #---------------------------------------------
MAXQ0$Estimate.V.C <- function(., n=100){
# runs n episodes of MAXQ0() using random initial state at each episode
# and updating the estimates for value functions and keeping track of the rewards
rewards <- c()
for(i in 1:n){
.$init(FALSE,alpha=.2, DF=1, epsilon=1/i)
# for each run generate a random state.
s0 <- c(sample(1:5,1),sample(1:5,1),sample(1:4,1),sample(1:4,1))
.$MAXQ0(7, s0) # always start from the root subtask
rewards <- c(rewards, .$gain)
if(i%%500==0){print(paste0(i," episodes are simulated"))}
}
return(rewards)
}
MAXQ0$get.rands <- function(.){ # returns a random state
return(c(sample(1:5,1),sample(1:5,1),sample(1:4,1),sample(1:4,1)))
}
#
#
#----------------------------------------------------------
# example of running one episode
e <- as.proto(MAXQ0)
e$init() # needs to be always called after the creation of the object
e$MAXQ0(7,e$get.rands())
#-----------------------------------------------------
# running n episodes and observing the resulting rewards and value estimates.
e$seed(12)
#-----------------------------------------------------
# running n episodes and observing the resulting rewards and value estimates.
e$add.seed(12)
e$init()
rewards <- e$Estimate.V.C(2000)
min(rewards); max(rewards) ;mean(rewards); sqrt(var(rewards))
plot(NA,NA, xlim = c(0,length(rewards)), ylim=c(min(rewards),max(rewards)),
xlab = "Episodes",ylab='total rewards')
lines(1:length(rewards),rewards)
Vest <- e$V
Cestim <- e$C
View(Vest)
taxi.env$decode(1)
taxi.env$decode(2)
taxi.env$decode(3)
taxi.env$s = taxi.env$decode(3)
taxi.env$render()
e$MAXQ0(7,c(1,1,1,1))
e$MAXQ0(7,c(1,1,1,2))
e$MAXQ0(7,c(1,1,1,3))
MAXQ0$MAXQ0 <- function(.,i, s){
# if it's primitive
.$taxi$s <- s
.$taxi$a <- i
.$taxi$render()
if(i <=6){
r <- .$taxi$step(i)
.$gain <- .$gain + r
.$V[i,.$taxi$encode(s)] <- (1-.$alpha) * .$V[i,.$taxi$encode(s)] + .$alpha * r
return(1)
}
# else if i is a composite action (subtask)
count = 0
while(!.$is.terminal(i,s)){
a <- .$Policy(i,s)
N <- .$MAXQ0(a,s)
if(all(.$taxi$ss==c(0,0,0,0))) {return(count)} # goal is achieved.
.$EvaluateMaxNode(i,.$taxi$ss)
.$C[[i]][.$taxi$encode(s),a] <- (1-.$alpha) * .$C[[i]][.$taxi$encode(s),a] +
.$alpha * (.$DF^N) * .$V[i,.$taxi$encode(.$taxi$ss)]
count <- count + N
s <- .$taxi$ss
.$taxi$update()
}
return(count)
}
MAXQ0$V <- Vest
MAXQ0$C <- Cestim
MAXQ0$MAXQ0(7,c(1,1,1,1))
MAXQ0$MAXQ0(7,c(1,1,1,2))
MAXQ0$MAXQ0(7,c(1,1,3,2))
MAXQ0$MAXQ0 <- function(.,i, s){
# if it's primitive
.$taxi$s <- s
.$taxi$a <- i
if(i <=6){
.$taxi$render()
r <- .$taxi$step(i)
.$gain <- .$gain + r
.$V[i,.$taxi$encode(s)] <- (1-.$alpha) * .$V[i,.$taxi$encode(s)] + .$alpha * r
return(1)
}
# else if i is a composite action (subtask)
count = 0
while(!.$is.terminal(i,s)){
a <- .$Policy(i,s)
N <- .$MAXQ0(a,s)
if(all(.$taxi$ss==c(0,0,0,0))) {return(count)} # goal is achieved.
.$EvaluateMaxNode(i,.$taxi$ss)
.$C[[i]][.$taxi$encode(s),a] <- (1-.$alpha) * .$C[[i]][.$taxi$encode(s),a] +
.$alpha * (.$DF^N) * .$V[i,.$taxi$encode(.$taxi$ss)]
count <- count + N
s <- .$taxi$ss
.$taxi$update()
}
return(count)
}
MAXQ0$MAXQ0(7,c(1,1,3,2))
