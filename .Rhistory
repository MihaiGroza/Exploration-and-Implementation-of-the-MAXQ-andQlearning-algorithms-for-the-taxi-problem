#     }else if(s[[4]]==3){return(c(12))
#     }else if(s[[4]]==4){return(c(13))}
#
#   }else {return(1:4)} # == else if(i is in 10:13)
# }
#
plot(0,0, xlim = c(0,length(rewards)), ylim=c(min(rewards),max(rewards)))
rewards
MAXQ0$Estimate.V.C <- function(., n=100){
rewards <- c()
for(i in 1:n){
.$init(FALSE, DF=.9, epsilon=.1)
# for each run generate a random state.
s0 <- c(sample(1:5,1),sample(1:5,1),sample(1:4,1),sample(1:4,1))
#s0 <- c(3,2,4,1)
# print(s0)
.$MAXQ0(7, s0)
rewards <- c(rewards, .$gain)
}
return(rewards)
}
#
#----------------------------------------------------------
#
#
learn <- as.proto(MAXQ0)
#s0 <- c(3,3,1,4)
#learn$init(FALSE)
#rrr <- learn$MAXQ0(7,s0)
# #learn <- MAXQ0$proto()
# learn$Policy(7,s0)
# learn$gain
#
#
learn$init()
rewards <- learn$Estimate.V.C(5000)
plot(1:length(rewards),rewards)
max(rewards)
min(rewards)
plot(0,0, xlim = c(0,length(rewards)), ylim=c(min(rewards),max(rewards)))
lines(1:length(rewards),rewards)
rewards
MAXQ0$Estimate.V.C <- function(., n=100){
rewards <- c()
for(i in 1:n){
.$init(FALSE, DF=1, epsilon=.1)
# for each run generate a random state.
s0 <- c(sample(1:5,1),sample(1:5,1),sample(1:4,1),sample(1:4,1))
#s0 <- c(3,2,4,1)
# print(s0)
.$MAXQ0(7, s0)
rewards <- c(rewards, .$gain)
}
return(rewards)
}
#
#----------------------------------------------------------
#
#
learn <- as.proto(MAXQ0)
#s0 <- c(3,3,1,4)
#learn$init(FALSE)
#rrr <- learn$MAXQ0(7,s0)
# #learn <- MAXQ0$proto()
# learn$Policy(7,s0)
# learn$gain
#
#
learn$init()
rewards <- learn$Estimate.V.C(5000)
plot(1:length(rewards),rewards)
max(rewards)
min(rewards)
MAXQ0$Estimate.V.C <- function(., n=100){
rewards <- c()
for(i in 1:n){
.$init(FALSE, DF=1, epsilon=1/i)
# for each run generate a random state.
s0 <- c(sample(1:5,1),sample(1:5,1),sample(1:4,1),sample(1:4,1))
#s0 <- c(3,2,4,1)
# print(s0)
.$MAXQ0(7, s0)
rewards <- c(rewards, .$gain)
}
return(rewards)
}
Vest <- learn$V
Cestim <- learn$C
View(Vest)
View(Cestim)
Cestim[[6]]
learn <- as.proto(MAXQ0)
#s0 <- c(3,3,1,4)
#learn$init(FALSE)
#rrr <- learn$MAXQ0(7,s0)
# #learn <- MAXQ0$proto()
# learn$Policy(7,s0)
# learn$gain
#
#
learn$init()
rewards <- learn$Estimate.V.C(5000)
plot(1:length(rewards),rewards)
max(rewards)
min(rewards)
plot(0,0, xlim = c(0,length(rewards)), ylim=c(min(rewards),max(rewards)))
lines(1:length(rewards),rewards)
MAXQ0$Estimate.V.C <- function(., n=100){
rewards <- c()
for(i in 1:n){
.$init(FALSE,alpha=.1, DF=.9, epsilon=1/i)
# for each run generate a random state.
s0 <- c(sample(1:5,1),sample(1:5,1),sample(1:4,1),sample(1:4,1))
#s0 <- c(3,2,4,1)
# print(s0)
.$MAXQ0(7, s0)
rewards <- c(rewards, .$gain)
}
return(rewards)
}
#
#----------------------------------------------------------
#
#
learn <- as.proto(MAXQ0)
#s0 <- c(3,3,1,4)
#learn$init(FALSE)
#rrr <- learn$MAXQ0(7,s0)
# #learn <- MAXQ0$proto()
# learn$Policy(7,s0)
# learn$gain
#
#
learn$init()
rewards <- learn$Estimate.V.C(5000)
plot(1:length(rewards),rewards)
max(rewards)
min(rewards)
MAXQ0$Estimate.V.C <- function(., n=100){
rewards <- c()
for(i in 1:n){
.$init(FALSE,alpha=.1, DF=.7, epsilon=1/i)
# for each run generate a random state.
s0 <- c(sample(1:5,1),sample(1:5,1),sample(1:4,1),sample(1:4,1))
#s0 <- c(3,2,4,1)
# print(s0)
.$MAXQ0(7, s0)
rewards <- c(rewards, .$gain)
}
return(rewards)
}
#
#----------------------------------------------------------
#
#
learn <- as.proto(MAXQ0)
#s0 <- c(3,3,1,4)
#learn$init(FALSE)
#rrr <- learn$MAXQ0(7,s0)
# #learn <- MAXQ0$proto()
# learn$Policy(7,s0)
# learn$gain
#
#
learn$init()
rewards <- learn$Estimate.V.C(5000)
plot(1:length(rewards),rewards)
max(rewards)
min(rewards)
vest2 <- learn$V
View(vest2)
MAXQ0$init <- function(.,reset.estimates=TRUE, alpha=.2,DF=1,epsilon=.01, seed=12){
set.seed(seed)
if(reset.estimates == TRUE){
.$V <- matrix(0,11,500)
.$C <-list()
for(i in 1:11){
.$C[[i]] <- matrix(0,500,11)
}
}
.$alpha <- alpha
.$DF <- DF
.$gain <- 0
.$epsilon <- epsilon
.$taxi <- as.proto(taxi.env)
}
MAXQ0$Estimate.V.C <- function(., n=100){
rewards <- c()
for(i in 1:n){
.$init(FALSE,alpha=.1, DF=.7, epsilon=1/i, seed=sample(1:100,1))
# for each run generate a random state.
s0 <- c(sample(1:5,1),sample(1:5,1),sample(1:4,1),sample(1:4,1))
#s0 <- c(3,2,4,1)
# print(s0)
.$MAXQ0(7, s0)
rewards <- c(rewards, .$gain)
}
return(rewards)
}
#
#
learn <- as.proto(MAXQ0)
learn$init()
rewards <- learn$Estimate.V.C(5000)
plot(1:length(rewards),rewards)
max(rewards)
min(rewards)
View(Vest)
View(Vest)
MAXQ0$Estimate.V.C <- function(., n=100){
rewards <- c()
.$s0s <- c()
for(i in 1:n){
.$init(FALSE,alpha=.1, DF=.7, epsilon=1/i)
# for each run generate a random state.
s0 <- c(sample(1:5,1),sample(1:5,1),sample(1:4,1),sample(1:4,1))
.$s0s <- c(.$s0s, .$taxi$encode(s0))
#s0 <- c(3,2,4,1)
# print(s0)
.$MAXQ0(7, s0)
rewards <- c(rewards, .$gain)
if(i%%1000==0){print(paste0(i," episodes are simulated"))}
}
return(rewards)
}
#
#----------------------------------------------------------
#
#
learn <- as.proto(MAXQ0)
learn$init()
rewards <- learn$Estimate.V.C(5000)
plot(1:length(rewards),rewards)
learn$s0s
library(nnet)
library(proto)
MAXQ0 <- proto(expr = {
V <- NA
C <- NA
epsilon <- NA
DF <- NA
alpha <- NA
#i <- NA # current subtask in action
#s <- NA # current state
taxi <- taxi.env$proto()
gain <- 0
init <- NA
is.terminal <- NA
Action.space <- NA
EvaluateMaxNode <- NA
Policy <- NA
MAXQ0 <- NA
Estimate.V.C <- NA
})
#----------------------------------
MAXQ0$init <- function(.,reset.estimates=TRUE, alpha=.2,DF=1,epsilon=.01, seed=12){
#set.seed(seed)
if(reset.estimates == TRUE){
.$V <- matrix(0,11,500)
.$C <-list()
for(i in 1:11){
.$C[[i]] <- matrix(0,500,11)
}
}
.$alpha <- alpha
.$DF <- DF
.$gain <- 0
.$epsilon <- epsilon
.$taxi <- as.proto(taxi.env)
}
#--------------------------------------
MAXQ0$is.terminal <- function(., i, s){
# returns TRUE if s is an end state for the subtask i.
# whenver in task 7(root), 9(dropoff), or 10(navig with a parent 9), terminate
# only of the episode is done
if(all(s==c(0,0,0,0))){return(TRUE)
}else if(any(i==c(8,10)) && s[3]==5){return(TRUE) # successfully picked up the passenger
}else if(any(i==c(9,11)) && s[3]!=5){return(TRUE) # attempting to drop off when passenger is not on board
}else if(any(i==1:6)){return(TRUE) # any primitive action is excuted once
}else if(i==10 && all(s[1:2]==.$taxi$loc.indx(s[3]))){return(TRUE)#taxi ready to pick up
}else if(i==11 && all(s[1:2]==.$taxi$loc.indx(s[4]))){return(TRUE)
}else {return(FALSE)}
}
#----------------------------------------------------------------
# MAXQ0$oldAction.space <- function(.,i,s){
#   # expects only i from 7 to 13. ie, composite actions [subtasks]
#   if(i==7){
#     if(s[3]=='5'){return(c(9))}else{return(c(8))}
#   }else if(i==8){
#     if(all(.$taxi$loc.indx(s[3])==s[1:2]) ){return(c(5))
#     }else if(s[3]==1){return(c(10))
#     }else if(s[3]==2){return(c(11))
#     }else if(s[3]==3){return(c(12))
#     }else if(s[3]==4){return(c(13))}
#   }else if(i==9){
#     if(all(.$taxi$loc.indx(s[4])==s[1:2]) ){return(c(6))
#     }else if(s[4]==1){return(c(10))
#     }else if(s[4]==2){return(c(11))
#     }else if(s[4]==3){return(c(12))
#     }else if(s[4]==4){return(c(13))}}
#   return(1:4) # == else if(i is in 10:13)
# }
#---------------------------------------------------
MAXQ0$Action.space <- function(.,i){
# expects only i from 7 to 13. ie, composite actions [subtasks]
if(i==7){return(c(8,9))
}else if(i==8){return(c(5,10))
}else if(i==9){return(c(6,11))
}else if(i==10 || i==11){return(1:4)}
}
#---------------------------------------------------
MAXQ0$EvaluateMaxNode <- function(., i, s){
if(any(i==1:6)){ # if i is a primitive max node
# no update required as it's already updated in the main function
return()
}else{ # if i is a composite max node: return max(in a) of Q[i,s,a]=V[a,s]+C[i,s,a]
actions <- .$Action.space(i)
#actions <- half.greedy.Action.space(i,s)
for(a in actions){.$EvaluateMaxNode(a, s)}
#print(c(encode(s),i,actions))
.$V[i,.$taxi$encode(s)] <- max(.$V[actions,.$taxi$encode(s)]+
.$C[[i]][.$taxi$encode(s),actions])
#print(.$V[i,.$taxi$encode(s)])
return()
}
}
#---------------------------------------------------
MAXQ0$Policy <- function(.,i,s){
# Epsilon-greedy policy
actions <- .$Action.space(i)
#actions <- half.greedy.Action.space(i,s)
if(runif(1) < .$epsilon){return(sample(actions,1))} # epsilon greedy
return(actions[which.max(.$V[actions,.$taxi$encode(s)]+
.$C[[i]][.$taxi$encode(s),actions])])
}
#---------------------------------------------------
MAXQ0$MAXQ0 <- function(.,i, s){
# if it's primitive
.$taxi$s <- s
.$taxi$a <- i
if(i <=6){
.$taxi$step()
.$gain <- .$gain + .$taxi$r
.$V[i,.$taxi$encode(s)] <- (1-.$alpha) * .$V[i,.$taxi$encode(s)] + .$alpha * .$taxi$r
return(1)
}else{
count = 0
while(!.$is.terminal(i,s)){
a <- .$Policy(i,s)
N <- .$MAXQ0(a,s)
if(all(.$taxi$ss==c(0,0,0,0))) {return(count)}
.$EvaluateMaxNode(i,.$taxi$ss)
.$C[[i]][.$taxi$encode(s),a] <- (1-.$alpha) * .$C[[i]][.$taxi$encode(s),a] +
.$alpha * (.$DF^N) * .$V[i,.$taxi$encode(.$taxi$ss)]
count <- count + N
s <- .$taxi$ss
.$taxi$s <- .$taxi$ss
}
#if(.$gain<=-1000){.$gain=-}
#.$taxi$render()
return(count)
}
}
# #---------------------------------------------
MAXQ0$Estimate.V.C <- function(., n=100){
rewards <- c()
.$s0s <- c()
set.seed(12)
for(i in 1:n){
.$init(FALSE,alpha=.1, DF=.7, epsilon=1/i)
# for each run generate a random state.
s0 <- c(sample(1:5,1),sample(1:5,1),sample(1:4,1),sample(1:4,1))
.$s0s <- c(.$s0s, .$taxi$encode(s0))
#s0 <- c(3,2,4,1)
# print(s0)
.$MAXQ0(7, s0)
rewards <- c(rewards, .$gain)
if(i%%1000==0){print(paste0(i," episodes are simulated"))}
}
return(rewards)
}
#
#----------------------------------------------------------
#
#
learn <- as.proto(MAXQ0)
#s0 <- c(3,3,1,4)
#learn$init(FALSE)
#rrr <- learn$MAXQ0(7,s0)
# #learn <- MAXQ0$proto()
# learn$Policy(7,s0)
# learn$gain
#
#
learn$init()
rewards <- learn$Estimate.V.C(5000)
plot(1:length(rewards),rewards)
max(rewards)
min(rewards)
plot(0,0, xlim = c(0,length(rewards)), ylim=c(min(rewards),max(rewards)))
lines(1:length(rewards),rewards)
Vest <- learn$V
Cestim <- learn$C
vest2 <- learn$V
MAXQ0$Estimate.V.C <- function(., n=100){
rewards <- c()
.$s0s <- c()
set.seed(12)
for(i in 1:n){
.$init(FALSE,alpha=.1, DF=.7, epsilon=1/i)
# for each run generate a random state.
s0 <- c(sample(1:5,1),sample(1:5,1),sample(1:4,1),sample(1:4,1))
.$s0s <- c(.$s0s, .$taxi$encode(s0))
#s0 <- c(3,2,4,1)
# print(s0)
.$MAXQ0(7, s0)
rewards <- c(rewards, .$gain)
if(i%%10==0){print(paste0(i," episodes are simulated"))}
}
return(rewards)
}
#
#----------------------------------------------------------
#
#
learn <- as.proto(MAXQ0)
#s0 <- c(3,3,1,4)
#learn$init(FALSE)
#rrr <- learn$MAXQ0(7,s0)
# #learn <- MAXQ0$proto()
# learn$Policy(7,s0)
# learn$gain
#
#
learn$init()
rewards <- learn$Estimate.V.C(5000)
plot(1:length(rewards),rewards)
max(rewards)
min(rewards)
Vest <- learn$V
learn$init()
rewards <- learn$Estimate.V.C(200)
plot(1:length(rewards),rewards)
max(rewards)
min(rewards)
plot(0,0, xlim = c(0,length(rewards)), ylim=c(min(rewards),max(rewards)))
lines(1:length(rewards),rewards)
learn$init()
rewards <- learn$Estimate.V.C(2000)
plot(1:length(rewards),rewards)
max(rewards)
min(rewards)
Vest <- learn$V
Cestim <- learn$C
save.image("D:/concordia/Fall20/STAT497 Reinforcement Learning/Final project/taxi_map/taxi-problem-with-MAXQ-and-Qlearning-in-R/2000run.RData")
View(Vest)
plot(0,0, xlim = c(0,length(rewards)), ylim=c(min(rewards),max(rewards)))
lines(1:length(rewards),rewards)
plot(0,0, xlim = c(0,length(rewards)), ylim=c(min(rewards),max(rewards)),
xlab = "Episodes",ylab='total rewards')
lines(1:length(rewards),rewards)
max(rewards)
min(rewards)
rewards
rewards[1500:2000]
mean(rewards)
var(rewards)
sqrt(var(rewards))
max(rewards)
min(rewards)
mean(rewards)
sqrt(var(rewards))
MAXQ0$Estimate.V.C <- function(., n=100){
rewards <- c()
.$s0s <- c()
set.seed(12)
for(i in 1:n){
.$init(FALSE,alpha=.2, DF=1, epsilon=1/i)
# for each run generate a random state.
s0 <- c(sample(1:5,1),sample(1:5,1),sample(1:4,1),sample(1:4,1))
.$s0s <- c(.$s0s, .$taxi$encode(s0))
#s0 <- c(3,2,4,1)
# print(s0)
.$MAXQ0(7, s0)
rewards <- c(rewards, .$gain)
#if(i%%10==0){print(paste0(i," episodes are simulated"))}
}
return(rewards)
}
#
#----------------------------------------------------------
#
#
learn <- as.proto(MAXQ0)
#s0 <- c(3,3,1,4)
#learn$init(FALSE)
#rrr <- learn$MAXQ0(7,s0)
# #learn <- MAXQ0$proto()
# learn$Policy(7,s0)
# learn$gain
#
#
learn$init()
rewards <- learn$Estimate.V.C(2000)
plot(1:length(rewards),rewards)
max(rewards)
min(rewards)
mean(rewards)
sqrt(var(rewards))
plot(0,0, xlim = c(0,length(rewards)), ylim=c(min(rewards),max(rewards)),
xlab = "Episodes",ylab='total rewards')
lines(1:length(rewards),rewards)
