#------------------------
firstVisitReturns <- function(pright, n=20000, theseed=12) {
# computes the expected rewards using first visit approach and n episodes
returnS <- c(0,0,0)
NS <- c(0,0,0)
for(e in 1:n){
sim <- SimulCorridorEpisodeFixedPolicy(pright, theseed+e)
stateseq <- sim[[1]]
actionseq  <- sim[[2]]
G <- 0
for(t in (length(stateseq)-1):1){
G <- G - 1
s <- stateseq[t]
if(!any(s==stateseq[1:(t-1)])){
returnS[s] <- returnS[s] + G
NS[s] <- NS[s] + 1
}
}
returnS[1] <- returnS[1] + G
NS[1] <- NS[1] + 1
}
return(returnS / NS)
}
firstVisitReturns(.95, 2000, theseed=7)
firstVisitReturns(.05, 2000, theseed=7)
#--------------------------------------------
# b
OneRunCorridor <- function(inittheta, alphasteptheta, nepis, theseed){
RewardEachEpisode <- NA
ProbRightEachEpisode <- NA
}
SimulCorridorEpisodeFixedPolicy <- function(pright, theseed=12){
set.seed(theseed)
stateseq <- c(1)
actionseq <- c()
repeat{
a <- sample(c(1,-1), 1, prob=c(pright,1-pright))
s <- tail(stateseq, 1) # current state
if(s==4){break} # terminal state
#------
actionseq <- c(actionseq, a)
if(s==2){a <- -a}
stateseq <- c(stateseq, max(s+a,1))
}
return(list(stateseq, actionseq))
}
#------------------------
firstVisitReturns <- function(pright, n=20000, theseed=12) {
# computes the expected rewards using first visit approach and n episodes
returnS <- c(0,0,0)
NS <- c(0,0,0)
for(e in 1:n){
sim <- SimulCorridorEpisodeFixedPolicy(pright, theseed+e)
stateseq <- sim[[1]]
actionseq  <- sim[[2]]
G <- 0
for(t in (length(stateseq)-1):1){
G <- G - 1
s <- stateseq[t]
if(!any(s==stateseq[1:(t-1)])){
returnS[s] <- returnS[s] + G
NS[s] <- NS[s] + 1
}
}
returnS[1] <- returnS[1] + G
NS[1] <- NS[1] + 1
}
return(returnS / NS)
}
firstVisitReturns(.95, 2000, theseed=7)
firstVisitReturns(.05, 2000, theseed=7)
#--------------------------------------------
# b
OneRunCorridor <- function(inittheta, alphasteptheta, nepis, theseed){
RewardEachEpisode <- NA
ProbRightEachEpisode <- NA
}
h <- c(0,0) # for a = 1, -1 and fixed s
exp(h)
piPreference <- function(a, theta){
h <- c(0,0) # for a = 1, -1 and fixed s
h[1] <- sum(theta * c(1,-1))
h[2] <- sum(theta * c(-1,1))
pi <- exp(h[a]) / sum(exp(h))
}
piPreference <- function(a, theta){
h <- c(0,0) # for a = 1, -1 and fixed s
h[1] <- sum(theta * c(1,-1))
h[2] <- sum(theta * c(-1,1))
return(exp(h[a]) / sum(exp(h)))
}
gradient <- function(a, theta){
h <- c(0,0) # for a = 1, -1 and fixed s
pi <- c(0,0)
h[1] <- sum(theta * c(1,0))
h[2] <- sum(theta * c(0,1))
pi[1] <- exp(h[1]) / sum(exp(h))
pi[2] <- exp(h[2]) / sum(exp(h))
grad <- ifelse(a==1, c(1,0), c(0,1)) - pi
return(grad)
}
OneRunCorridor <- function(inittheta, alphasteptheta, nepis, theseed){
RewardEachEpisode <- rep(0,nepis)
ProbRightEachEpisode <- rep(0, nepis)
theta <- inittheta
for (e in 1:nepis){
sim <- SimulCorridorEpisodeFixedPolicy(piRight(theta), theseed+e)
stateseq <- sim[[1]]
actionseq  <- sim[[2]]
nsteps <- length(stateseq)
RewardEachEpisode[e] <- -1 * (nsteps-1)
for(t in  1:(nsteps-1)){
G <- -1 * (nsteps-t)
theta <- theta + alphasteptheta + G + gradient(actionseq[t], theta)
}
ProbRightEachEpisode[e] <- piRight(theta)
}
}
OneRunCorridor <- function(inittheta, alphasteptheta, nepis, theseed){
RewardEachEpisode <- rep(0,nepis)
ProbRightEachEpisode <- rep(0, nepis)
theta <- inittheta
for (e in 1:nepis){
sim <- SimulCorridorEpisodeFixedPolicy(piRight(theta), theseed+e)
stateseq <- sim[[1]]
actionseq  <- sim[[2]]
nsteps <- length(stateseq)
RewardEachEpisode[e] <- -1 * (nsteps-1)
for(t in  1:(nsteps-1)){
G <- -1 * (nsteps-t)
theta <- theta + alphasteptheta + G + gradient(actionseq[t], theta)
}
ProbRightEachEpisode[e] <- piRight(theta)
}
}
array(0,c(2,3,1))
array(0,c(2,2,3))
w = array(0,c(2,2,3))
w[1,2]
w[1,2,]
w[1,3,]
w[1,2,3]
w[1,,3]
2e-12
SimulCorridorEpisodeFixedPolicy <- function(pright, theseed=12){
set.seed(theseed)
stateseq <- c(1)
actionseq <- c()
repeat{
a <- sample(c(1,-1), 1, prob=c(pright,1-pright))
s <- tail(stateseq, 1) # current state
if(s==4){break} # terminal state
#------
actionseq <- c(actionseq, a)
if(s==2){a <- -a}
stateseq <- c(stateseq, max(s+a,1))
}
return(list(stateseq, actionseq))
}
#------------------------
firstVisitReturns <- function(pright, n=20000, theseed=12) {
# computes the expected rewards using first visit approach and n episodes
returnS <- c(0,0,0)
NS <- c(0,0,0)
for(e in 1:n){
sim <- SimulCorridorEpisodeFixedPolicy(pright, theseed+e)
stateseq <- sim[[1]]
actionseq  <- sim[[2]]
G <- 0
for(t in (length(stateseq)-1):1){
G <- G - 1
s <- stateseq[t]
if(!any(s==stateseq[1:(t-1)])){
returnS[s] <- returnS[s] + G
NS[s] <- NS[s] + 1
}
}
returnS[1] <- returnS[1] + G
NS[1] <- NS[1] + 1
}
return(returnS / NS)
}
firstVisitReturns(.95, 2000, theseed=7)
firstVisitReturns(.05, 2000, theseed=7)
#--------------------------------------------
# b
piRight <-  function(theta){
h <- c(0,0) # for a = 1, -1 and fixed s
pi <- c(0,0)
h[1] <- sum(theta * c(1,0))
h[2] <- sum(theta * c(0,1))
return(exp(h[1]) / sum(exp(h)))
}
gradient <- function(a, theta){
h <- c(0,0) # for a = 1, -1 and fixed s
pi <- c(0,0)
h[1] <- sum(theta * c(1,0))
h[2] <- sum(theta * c(0,1))
pi[1] <- exp(h[1]) / sum(exp(h))
pi[2] <- exp(h[2]) / sum(exp(h))
grad <- ifelse(a==1, c(1,0), c(0,1)) - pi
return(grad)
}
# part C
OneRunCorridor <- function(inittheta, alphasteptheta, nepis, theseed){
RewardEachEpisode <- rep(0,nepis)
ProbRightEachEpisode <- rep(0, nepis)
theta <- inittheta
for (e in 1:nepis){
sim <- SimulCorridorEpisodeFixedPolicy(piRight(theta), theseed+e)
stateseq <- sim[[1]]
actionseq  <- sim[[2]]
nsteps <- length(stateseq)
RewardEachEpisode[e] <- -1 * (nsteps-1)
for(t in  1:(nsteps-1)){
G <- -1 * (nsteps-t)
theta <- theta + alphasteptheta + G + gradient(actionseq[t], theta)
}
ProbRightEachEpisode[e] <- piRight(theta)
}
return(list(RewardEachEpisode, ProbRightEachEpisode))
}
# part D
SeveralRuns <- function(runs, inittheta, alpha, nepis=1000, theseed=12){
returnmat <- matrix(NA, runs, nepis)
probrightmat <- matrix(NA, runs, nepis)
for(r in 1:runs){
onerun <- OneRunCorridor(inittheta, alpha, nepis, theseed)
returnmat[r,] <- onerun[[1]]
probrightmat[r,] <- onerun[[2]]
}
return(list(returnmat, probrightmat))
}
#-----
inittheta <- c( log((1/.95) - 1),0 )
returnmat <- array(NA, c(100,1000,3))
probrightmat <- array(NA, c(100,1000,3))
sevrn <- SeveralRuns(100, inittheta, 2e-12, 1000, theseed=12)
returnmat[,,1] <- sevrn[[1]]
probrightmat[,,1] <- sevrn[[2]]
sevrn <- SeveralRuns(100, inittheta, 2e-13, 1000, theseed=12)
returnmat[,,2] <- sevrn[[1]]
probrightmat[,,2] <- sevrn[[2]]
sevrn <- SeveralRuns(100, inittheta, 2e-14, 1000, theseed=12)
returnmat[,,3] <- sevrn[[1]]
probrightmat[,,3] <- sevrn[[2]]
piRigh(inittheta)
piRight <-  function(theta){
h <- c(0,0) # for a = 1, -1 and fixed s
pi <- c(0,0)
h[1] <- sum(theta * c(1,0))
h[2] <- sum(theta * c(0,1))
return(exp(h[1]) / sum(exp(h)))
}
piRigh(inittheta)
piRight(inittheta)
piRight(inittheta)
sample(c(-1,1), 1, prob=c(piRight(inittheta), 1-piRight(inittheta)))
gradient <- function(a, theta){
h <- c(0,0) # for a = 1, -1 and fixed s
pi <- c(0,0)
h[1] <- sum(theta * c(1,0))
h[2] <- sum(theta * c(0,1))
pi[1] <- exp(h[1]) / sum(exp(h))
pi[2] <- exp(h[2]) / sum(exp(h))
grad <- ifelse(a==1, c(1,0), c(0,1)) - pi
return(grad)
}
gradient(1,inittheta)
OneRunCorridor(inittheta, .02, 5, 12)
OneRunCorridor(inittheta, .02, 1, 12)
alphasteptheta=.02
e=2
theta <- inittheta
sim <- SimulCorridorEpisodeFixedPolicy(piRight(theta), theseed+e)
stateseq <- sim[[1]]
actionseq  <- sim[[2]]
nsteps <- length(stateseq)
length(stateseq)
t=1
theta + alphasteptheta + G + gradient(actionseq[t], theta)
theta <- theta + alphasteptheta + G + gradient(actionseq[t], theta)
theta
debug(OneRunCorridor)
OneRunCorridor(inittheta, .02, 5, 12)
OneRunCorridor(inittheta, .02, 5, 12)
theta + alphasteptheta + G + gradient(actionseq[t], theta)
theta + alphasteptheta + G + gradient(actionseq[t], theta)
theta + alphasteptheta + G + gradient(actionseq[t], theta)
theta + alphasteptheta + G + gradient(actionseq[t], theta)
theta + alphasteptheta + G + gradient(actionseq[t], theta)
theta + alphasteptheta + G + gradient(actionseq[t], theta)
gradient(actionseq[t], theta)
actionseq[t]
gradient(-1, c(-748, -748))
debug(gradient)
gradient(-1, c(-748,-748))
sum(exp(h))
exp(h)
exp(-748)
SimulCorridorEpisodeFixedPolicy <- function(pright, theseed=12){
set.seed(theseed)
stateseq <- c(1)
actionseq <- c()
repeat{
a <- sample(c(1,-1), 1, prob=c(pright,1-pright))
s <- tail(stateseq, 1) # current state
if(s==4){break} # terminal state
#------
actionseq <- c(actionseq, a)
if(s==2){a <- -a}
stateseq <- c(stateseq, max(s+a,1))
}
return(list(stateseq, actionseq))
}
#------------------------
firstVisitReturns <- function(pright, n=20000, theseed=12) {
# computes the expected rewards using first visit approach and n episodes
returnS <- c(0,0,0)
NS <- c(0,0,0)
for(e in 1:n){
sim <- SimulCorridorEpisodeFixedPolicy(pright, theseed+e)
stateseq <- sim[[1]]
actionseq  <- sim[[2]]
G <- 0
for(t in (length(stateseq)-1):1){
G <- G - 1
s <- stateseq[t]
if(!any(s==stateseq[1:(t-1)])){
returnS[s] <- returnS[s] + G
NS[s] <- NS[s] + 1
}
}
returnS[1] <- returnS[1] + G
NS[1] <- NS[1] + 1
}
return(returnS / NS)
}
firstVisitReturns(.95, 2000, theseed=7)
firstVisitReturns(.05, 2000, theseed=7)
#--------------------------------------------
# b
piRight <-  function(theta){
h <- c(0,0) # for a = 1, -1 and fixed s
pi <- c(0,0)
h[1] <- sum(theta * c(1,0))
h[2] <- sum(theta * c(0,1))
return(exp(h[1]) / sum(exp(h)))
}
gradient <- function(a, theta){
h <- c(0,0) # for a = 1, -1 and fixed s
pi <- c(0,0)
h[1] <- sum(theta * c(1,0))
h[2] <- sum(theta * c(0,1))
pi[1] <- exp(h[1]) / sum(exp(h))
pi[2] <- exp(h[2]) / sum(exp(h))
grad <- ifelse(a==1, c(1,0), c(0,1)) - pi
return(grad)
}
# part C
OneRunCorridor <- function(inittheta, alphasteptheta, nepis, theseed){
RewardEachEpisode <- rep(0,nepis)
ProbRightEachEpisode <- rep(0, nepis)
theta <- inittheta
for (e in 1:nepis){
sim <- SimulCorridorEpisodeFixedPolicy(piRight(theta), theseed+e)
stateseq <- sim[[1]]
actionseq  <- sim[[2]]
nsteps <- length(stateseq)
RewardEachEpisode[e] <- -1 * (nsteps-1)
for(t in  1:(nsteps-1)){
G <- -1 * (nsteps-t)
theta <- theta + alphasteptheta * G * gradient(actionseq[t], theta)
}
ProbRightEachEpisode[e] <- piRight(theta)
}
return(list(RewardEachEpisode, ProbRightEachEpisode))
}
# part D
SeveralRuns <- function(runs, inittheta, alpha, nepis=1000, theseed=12){
returnmat <- matrix(NA, runs, nepis)
probrightmat <- matrix(NA, runs, nepis)
for(r in 1:runs){
onerun <- OneRunCorridor(inittheta, alpha, nepis, theseed)
returnmat[r,] <- onerun[[1]]
probrightmat[r,] <- onerun[[2]]
}
return(list(returnmat, probrightmat))
}
#-----
inittheta <- c( log((1/.95) - 1),0 )
returnmat <- array(NA, c(100,1000,3))
probrightmat <- array(NA, c(100,1000,3))
sevrn <- SeveralRuns(100, inittheta, 2e-12, 1000, theseed=12)
returnmat[,,1] <- sevrn[[1]]
probrightmat[,,1] <- sevrn[[2]]
sevrn <- SeveralRuns(100, inittheta, 2e-13, 1000, theseed=12)
returnmat[,,2] <- sevrn[[1]]
probrightmat[,,2] <- sevrn[[2]]
sevrn <- SeveralRuns(100, inittheta, 2e-14, 1000, theseed=12)
returnmat[,,3] <- sevrn[[1]]
probrightmat[,,3] <- sevrn[[2]]
Q
Q
Q
source('D:/concordia/Fall20/STAT497 Reinforcement Learning/Assignments/Assignment3/Q2.R')
SimulCorridorEpisodeFixedPolicy <- function(pright, theseed=12){
set.seed(theseed)
stateseq <- c(1)
actionseq <- c()
repeat{
a <- sample(c(1,-1), 1, prob=c(pright,1-pright))
s <- tail(stateseq, 1) # current state
if(s==4){break} # terminal state
#------
actionseq <- c(actionseq, a)
if(s==2){a <- -a}
stateseq <- c(stateseq, max(s+a,1))
}
return(list(stateseq, actionseq))
}
#------------------------
firstVisitReturns <- function(pright, n=20000, theseed=12) {
# computes the expected rewards using first visit approach and n episodes
returnS <- c(0,0,0)
NS <- c(0,0,0)
for(e in 1:n){
sim <- SimulCorridorEpisodeFixedPolicy(pright, theseed+e)
stateseq <- sim[[1]]
actionseq  <- sim[[2]]
G <- 0
for(t in (length(stateseq)-1):1){
G <- G - 1
s <- stateseq[t]
if(!any(s==stateseq[1:(t-1)])){
returnS[s] <- returnS[s] + G
NS[s] <- NS[s] + 1
}
}
returnS[1] <- returnS[1] + G
NS[1] <- NS[1] + 1
}
return(returnS / NS)
}
firstVisitReturns(.95, 2000, theseed=7)
firstVisitReturns(.05, 2000, theseed=7)
#--------------------------------------------
# b
piRight <-  function(theta){
h <- c(0,0) # for a = 1, -1 and fixed s
pi <- c(0,0)
h[1] <- sum(theta * c(1,0))
h[2] <- sum(theta * c(0,1))
return(exp(h[1]) / sum(exp(h)))
}
gradient <- function(a, theta){
h <- c(0,0) # for a = 1, -1 and fixed s
pi <- c(0,0)
h[1] <- sum(theta * c(1,0))
h[2] <- sum(theta * c(0,1))
pi[1] <- exp(h[1]) / sum(exp(h))
pi[2] <- exp(h[2]) / sum(exp(h))
grad <- ifelse(a==1, c(1,0), c(0,1)) - pi
return(grad)
}
# part C
OneRunCorridor <- function(inittheta, alphasteptheta, nepis, theseed){
RewardEachEpisode <- rep(0,nepis)
ProbRightEachEpisode <- rep(0, nepis)
theta <- inittheta
for (e in 1:nepis){
sim <- SimulCorridorEpisodeFixedPolicy(piRight(theta), theseed+e)
stateseq <- sim[[1]]
actionseq  <- sim[[2]]
nsteps <- length(stateseq)
RewardEachEpisode[e] <- -1 * (nsteps-1)
for(t in  1:(nsteps-1)){
G <- -1 * (nsteps-t)
theta <- theta + alphasteptheta * G * gradient(actionseq[t], theta)
}
ProbRightEachEpisode[e] <- piRight(theta)
}
return(list(RewardEachEpisode, ProbRightEachEpisode))
}
# part D
SeveralRuns <- function(runs, inittheta, alpha, nepis=1000, theseed=12){
returnmat <- matrix(NA, runs, nepis)
probrightmat <- matrix(NA, runs, nepis)
for(r in 1:runs){
onerun <- OneRunCorridor(inittheta, alpha, nepis, theseed)
returnmat[r,] <- onerun[[1]]
probrightmat[r,] <- onerun[[2]]
}
return(list(returnmat, probrightmat))
}
#-----
inittheta <- c( log((1/.95) - 1),0 )
returnmat <- array(NA, c(100,1000,3))
probrightmat <- array(NA, c(100,1000,3))
sevrn <- SeveralRuns(100, inittheta, 2e-12, 1000, theseed=12)
returnmat[,,1] <- sevrn[[1]]
probrightmat[,,1] <- sevrn[[2]]
sevrn <- SeveralRuns(100, inittheta, 2e-13, 1000, theseed=12)
returnmat[,,2] <- sevrn[[1]]
probrightmat[,,2] <- sevrn[[2]]
sevrn <- SeveralRuns(100, inittheta, 2e-14, 1000, theseed=12)
returnmat[,,3] <- sevrn[[1]]
probrightmat[,,3] <- sevrn[[2]]
inittheta
a <- OneRunCorridor(inittheta, 2e-12, 1000, 12)
a[[1]]
a[[2]]
