
\documentclass[twoside,twocolumn]{article}
\usepackage{datetime}
\usepackage{blindtext,qtree,amsmath} % Package to generate dummy text throughout this template 
\usepackage{graphicx,floatrow}

\usepackage[sc]{mathpazo} % Use the Palatino font
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\linespread{1.05} % Line spacing - Palatino needs more space between lines
\usepackage{microtype} % Slightly tweak font spacing for aesthetics

\usepackage[english]{babel} % Language hyphenation and typographical rules

\usepackage[hmarginratio=1:1,top=32mm,columnsep=20pt]{geometry} % Document margins
\usepackage[hang, small,labelfont=bf,up,textfont=it,up]{caption} % Custom captions under/above floats in tables or figures
\usepackage{booktabs} % Horizontal rules in tables

\usepackage{lettrine} % The lettrine is the first enlarged letter at the beginning of the text

\usepackage{enumitem} % Customized lists
\setlist[itemize]{noitemsep} % Make itemize lists more compact

\usepackage{abstract} % Allows abstract customization
\renewcommand{\abstractnamefont}{\normalfont\bfseries} % Set the "Abstract" text to bold
\renewcommand{\abstracttextfont}{\normalfont\small\itshape} % Set the abstract itself to small italic text

\usepackage{titlesec} % Allows customization of titles
\renewcommand\thesection{\Roman{section}} % Roman numerals for the sections
\renewcommand\thesubsection{\roman{subsection}} % roman numerals for subsections
\titleformat{\section}[block]{\large\scshape\centering}{\thesection.}{1em}{} % Change the look of the section titles
\titleformat{\subsection}[block]{\large}{\thesubsection.}{1em}{} % Change the look of the section titles

\usepackage{fancyhdr} % Headers and footers
\pagestyle{fancy} % All pages have headers and footers
\fancyhead{} % Blank out the default header
\fancyfoot{} % Blank out the default footer
%\fancyhead[C]{Running title $\bullet$ May 2016 $\bullet$ Vol. XXI, No. 1} % Custom header text
\fancyfoot[RO,LE]{\thepage} % Custom footer text

\usepackage{titling} % Customizing the title section

\usepackage{hyperref} % For hyperlinks in the PDF

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\setlength{\droptitle}{-4\baselineskip} % Move the title up

\pretitle{\begin{center}\Huge\bfseries} % Article title formatting
\posttitle{\end{center}} % Article title closing formatting
\title{Implementing MAXQ and Qlearning methods in R and applying them to the taxi problem \thanks{Reinforcement learning undergraduate class project}}% Article title
\author{%
\textsc{Mihai Groza} \\[1ex] % Your name
\normalsize Concordia University \\ % Your institution
\normalsize \href{mailto:fill this}{fill this} % Your email address
\and % Uncomment if 2 authors are required, duplicate these 4 lines if more
\textsc{Khaled Fouda} \\[1ex] % Second author's name
\normalsize Concordia University \\ % Second author's institution
\normalsize \href{mailto:khaledsfouda@gmail.com}{khaledsfouda@gmail.com} % Second author's email address
}

%\newdate{date}{02}{02}{2020}
%\date{\displaydate{date}}
%\date{\today} % Leave empty to omit a date
\renewcommand{\maketitlehookd}{%
\begin{abstract}
last thing to do.
\end{abstract}
}

%----------------------------------------------------------------------------------------

\begin{document}

% Print the title
\maketitle

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\section{Introduction}
later.
%------------------------------------------------
\section{Environment}
%\begin{center}
\begin{figure}
\centering
\includegraphics[scale=.4]{taxi.png} 
\caption{generated by render(s)}
\end{figure}
%\end{center}
The taxi's - shown as \# - goal is to first pick up the passenger from one of the four places (R,Y,G,B) where the current location of the passenger is shown in green. After picking up the customer, the next and final goal is to drive him to his destination which is one of the four places (R,Y,G,B) and the destination is shown in red.\\

The state is defined as a set (taxi\_row, taxi\_column, passenger\_location, destination\_location) where:\\
taxi\_row and taxi\_column take values from 1 to 5 as we have 5x5 grid,\\
passenger\_location and destination\_location take values [1,2,3,4] for [R,Y,G,B], furthermore, if the passenger is in the taxi then they takes a location 5.\\
That being said, we have $5*5*5*4=500$  different states.\\

The set of actions available is ( North, South, East, Wast ) with ids 1 to 6.\\

If the taxi successfully dropped off the passenger then the episode is over and they receive a reward of +20, if they dropped the passenger at a wrong location or before picking them up first then they receive a reward of -10 and continue the episode to drop the passenger at the right location (ie. no change in the state). Similarly if they attempted to pick the passenger at the wrong place. Otherwise, they receive a reward of -1 for each step taken. Note that hitting the wall results in a reward of -1 and no change in the state.\\

I have implemented the following functions for the environment:\\
\begin{itemize}
\item render(state) : returns nothing and prints out the environment at the current state.\\

\item encode(s) : maps each state to an integer between 1 and 500.\\

\item decode(i): The inverse of encode(s). it takes an integer between 1 and 500 and returns the corresponding state.\\

\item loc.indx(i): maps the four pick/drop locations (1,2,3,4) to a (row,column) set.\\

\item hitting.wallQ(r,c,a): returns True if taking the action (a) at the location (r,c) would results in hitting the wall.\\

\item step(s,a) returns (s',r) at state s, it takes the action a, observes the reward r and the next state s' 
\end{itemize}
 
%-------------------------------------------------
\section{MAXQ method}
In our problem, the agent's goal is not only reaching the destination but is to also pick up the customer first. From this we define the following sequence of subtasks needed to reach the goal. \\

\textbf{Reach the customer $\Rightarrow$ Pick them up $\Rightarrow$ Drive to the destination $\Rightarrow$ drop them off} \\

MAXQ algorithm is one  of the hierarchical reinforcement learning methods which guarantees ultimate goal satisfaction only if a sequence of sub-goals was satisfied. \\
One the other hand, the algorithm's performance depends on how the structure of hierarchy is defined. MAXQ0 provides no way of learning that structure.\\
In MAXQ there are two types of actions: a primitive action which is the action that change the state of the environment and composite action which is a sub-task. For our Implementations, we defined the following actions:
\begin{itemize}
\item Primitive actions:
\begin{enumerate}
  \item Move one step north (N)
  \item Move one step south (S)
  \item Move one step east (E)
  \item Move one step south (W)
  \item pick-up the passenger
  \item drop-off the passenger
\end{enumerate}
\item subtasks actions:
\begin{enumerate}
  \setcounter{enumi}{6}
  \item Root - explained below -
  \item reach the customer and pick them up (GET)
  \item drive to the destination and drop them off (PUT)
  \item drive to the customer (Navigate\_C)  
  \item drive to the destination (Navigate\_D)
\end{enumerate}
\end{itemize}
and Their hierarchical structure is defined as follows:
\begin{center}
\Tree[.Root 
		[.GET  [.Pick ] [.Navigate\_C N S E W ] ]
        [.PUT  [.Navigate\_D N S E W ] [.Drop ] ] ]

\end{center} 
Where - starting at root - the agent has to recursively go down the graph satisfying each composite action. The root is satisfied if both get and put are satisfied and hence the episode ends. At each subtask i, the agent needs to repeatedly choose an action until the task is completed and hence we go back to the parent subtask. \\
For each action, a list of Terminal states is defined. If an action is at a terminal state, it no longer needs move down the tree and it returns back to it's parent subtask. For example, at Navigate\_C, it repeatedly choose one of the four actions [N,S,E,W] until it reaches the location of the customer. When that happens, we say that Navigate\_C reached a terminal state and it stops executing it's subtask and it reports back to it's parent "GET" who either choose an action "Pick" or "Navigate\_C" or if it's at a terminal state, it reports back to root and so on. The only terminal state for root is when both "GET" and "PUT" reached their terminal states and the episode is over.  

\subsection*{value function Estimates}

Let $(i,s)$ define the state of the environment at time $t$ where $i$ is the subtask running. Let also $a$ be the action (sub-subtask) taken by subtask i at time t.
Define 
$$ C^\pi(i,s,a) = (1-\alpha)C^\pi(i,s,a) + \gamma^N * V[i,s'] $$
\[
    V^\pi(i,s) =
    \begin{cases}
        (1-\alpha)*V^\pi(i,s) + \alpha * Reward(s'|s,i) & \text{if i is primitive}\\
        max_{a\in A(i)} V^\pi(a,s) + C^\pi(i,s,a) & \text{if i is a subtask}
    \end{cases}
\]\\

where $\alpha$ is the learning rate, $\gamma$ is the discounting factor, $N$ is the number of times the subtask i  executed actions before reaching the terminal state, and $A(i)$ is the set of available actions to  subtask i. \\

\subsection*{Policy}
\subsection*{Implementation}


sefsef\\
sefsef\\
sefes\\
sfseef\
sfesef\\
sefsef\\
sefes\\
sfseef\
sfesef\\
sefsef\\
sefes\\
sfseef\
sfesef\\
sefsef\\
sefes\\
sfseef\
sfesef\\
sefsef\\
sefes\\
sfseef\
sfesef\\
sefes\\
sfseef\
sfesef\\
sefsef\\
sefes\\
sfseef\
sfesef\\
sefsef\\
sefes\\
sfseef\
sfesef\\
sefsef\\
sefes\\
sfseef\
sfesef\\
sefsef\\
sefes\\
sfseef\
sfesef\\
%------------------------------------------------
\section{Qlearning method}
\section{Results}
\section{Similar projects}
\section{Final thoughts}

%----------------------------------------------------------------------------------------
%	REFERENCE LIST
%----------------------------------------------------------------------------------------

\begin{thebibliography}{99} % Bibliography - this is intentionally simple in this template

\bibitem[Figueredo and Wolf, 2009]{Figueredo:2009dg}
Figueredo, A.~J. and Wolf, P. S.~A. (2009).
\newblock Assortative pairing and life history strategy - a cross-cultural
  study.
\newblock {\em Human Nature}, 20:317--330.
 
\end{thebibliography}

%----------------------------------------------------------------------------------------

\end{document}
